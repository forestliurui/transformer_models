#!/bin/bash
#
#BSUB -a openmpi            		# tell lsf this is an openmpi job
#BSUB -W 120:0               		# total wall-clock time (120 h = 5 days)
#BSUB -n 1                    		# number of tasks in job
#BSUB -R "span[ptile=1]" 		# limit 20 processes per node. See note above about HT
#BSUB -R rusage[mem=10000] 	# amount of total memory in MB for all processes
#BSUB -R "affinity[core(10):cpubind=core:distribute=balance]"
#BSUB -R "select[hname=='gpu-cn010']" # gpu-cn001 always give me cuda OOM error
#BSUB -M 10000			# amount of memory in MB per process
#BSUB -J electra_pretraining			# job name
#BSUB -e errors.%J      		# error file name in which %J is replaced by the job ID
#BSUB -o output.%J      		# output file name in which %J is replaced by the job ID
#BSUB -q gpu_p100			# choose the queue to use: see list below
#BSUB -B 				# email job start notification
#BSUB -N 				# email job end notification
#BSUB -u ruixliu@umich.edu	# email address to send notifications

cd ../PyTorch
CUDA_VISIBLE_DEVICES=2,3 mpirun -n 1 python train.py --config_file="electra-small-single-node-rui.json" --config_file_path="/gpfs/gpfs0/groups/mozafari/ruixliu/code/transformer_models/pretrain/configs" --train_path="/gpfs/gpfs0/groups/mozafari/ruixliu/data/enwiki/bin_shards" --validation_path="/gpfs/gpfs0/groups/mozafari/ruixliu/data/enwiki/validation" --output_dir="/gpfs/gpfs0/groups/mozafari/ruixliu/tmp/models" --use_multigpu_with_single_device_per_process=False --train_batch_size=128 --log_steps=10 --accumulate_gradients=False --max_seq_length=128 --model=electra
